{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-16T14:00:37.129453Z","iopub.execute_input":"2024-06-16T14:00:37.129818Z","iopub.status.idle":"2024-06-16T14:00:37.135730Z","shell.execute_reply.started":"2024-06-16T14:00:37.129790Z","shell.execute_reply":"2024-06-16T14:00:37.134839Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7","metadata":{"execution":{"iopub.status.busy":"2024-06-16T14:00:37.144711Z","iopub.execute_input":"2024-06-16T14:00:37.144970Z","iopub.status.idle":"2024-06-16T14:00:49.707512Z","shell.execute_reply.started":"2024-06-16T14:00:37.144947Z","shell.execute_reply":"2024-06-16T14:00:49.706543Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import LoraConfig, PeftModel\nfrom trl import SFTTrainer","metadata":{"execution":{"iopub.status.busy":"2024-06-16T14:00:49.709601Z","iopub.execute_input":"2024-06-16T14:00:49.709916Z","iopub.status.idle":"2024-06-16T14:00:49.715320Z","shell.execute_reply.started":"2024-06-16T14:00:49.709888Z","shell.execute_reply":"2024-06-16T14:00:49.714464Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# model which is needed to be trained\nmodel_name = \"NousResearch/Llama-2-7b-chat-hf\"\n\n# The dataset for training\ndataset_name = \"mlabonne/guanaco-llama2-1k\"\n\n# fine tuned model name\nnew_model = \"Llama-2-7b-chat-finetune\"\n\n# QLoRA parameters\n\n# LoRA attention dimension\nlora_r = 64\n\n# Alpha parameter for LoRA scaling\nlora_alpha = 16\n\n# Dropout probability for LoRA layers\nlora_dropout = 0.1\n\n# bitsandbytes parameters for quantisation\n\n# Activate 4-bit precision base model loading\nuse_4bit = True\n\n# Compute dtype for 4-bit base models\nbnb_4bit_compute_dtype = \"float16\"\n\n# Quantization type\nbnb_4bit_quant_type = \"nf4\"\n\n# Activate nested quantization for 4-bit base models (double quantization)\nuse_nested_quant = False\n\n# TrainingArguments parameters\n\n# Output directory where the model predictions and checkpoints will be stored\noutput_dir = \"./results\"\n\n# Number of training epochs\nnum_train_epochs = 1\n\n# Enable fp16/bf16 training (set bf16 to True with an A100)\nfp16 = False\nbf16 = False\n\n# Batch size per GPU for training\nper_device_train_batch_size = 4\n\n# Batch size per GPU for evaluation\nper_device_eval_batch_size = 4\n\n# Number of update steps to accumulate the gradients for\ngradient_accumulation_steps = 1\n\n# Enable gradient checkpointing\ngradient_checkpointing = True\n\n# Maximum gradient normal (gradient clipping)\nmax_grad_norm = 0.3\n\n# Initial learning rate (AdamW optimizer)\nlearning_rate = 2e-4\n\n# Weight decay to apply to all layers except bias/LayerNorm weights\nweight_decay = 0.001\n\n# Optimizer to use\noptim = \"paged_adamw_32bit\"\n\n# Learning rate schedule\nlr_scheduler_type = \"cosine\"\n\n# Number of training steps \nmax_steps = -1\n\n# Ratio of steps for a linear warmup`\nwarmup_ratio = 0.03\n\n# Group sequences into batches with same length\n# Saves memory and speeds up training considerably\ngroup_by_length = True\n\n# Save checkpoint every X updates steps\nsave_steps = 0\n\n# Log every X updates steps\nlogging_steps = 25\n\n# SFT parameters\n\n# Maximum sequence length to use\nmax_seq_length = None\n\n# Pack multiple short examples in the same input sequence to increase efficiency\npacking = False\n\n# Load the entire model on the GPU 0\ndevice_map = {\"\": 0}","metadata":{"execution":{"iopub.status.busy":"2024-06-16T14:00:49.716587Z","iopub.execute_input":"2024-06-16T14:00:49.716870Z","iopub.status.idle":"2024-06-16T14:00:49.728270Z","shell.execute_reply.started":"2024-06-16T14:00:49.716845Z","shell.execute_reply":"2024-06-16T14:00:49.727425Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Load dataset (you can process it here)\ndataset = load_dataset(dataset_name, split=\"train\")\n\n# Load tokenizer and model with QLoRA configuration\ncompute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=use_4bit,\n    bnb_4bit_quant_type=bnb_4bit_quant_type,\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=use_nested_quant,\n)\n\n# Check GPU compatibility with bfloat16\nif compute_dtype == torch.float16 and use_4bit:\n    major, _ = torch.cuda.get_device_capability()\n    if major >= 8:\n        print(\"=\" * 80)\n        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n        print(\"=\" * 80)\n\n# Load base model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=device_map\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\n# Load LLaMA tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n\n# Load LoRA configuration\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\n# Set training parameters\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n    fp16=fp16,\n    bf16=bf16,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=group_by_length,\n    lr_scheduler_type=lr_scheduler_type,\n    report_to=\"tensorboard\"\n)\n\n# Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=packing,\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-16T14:00:49.731241Z","iopub.execute_input":"2024-06-16T14:00:49.731922Z","iopub.status.idle":"2024-06-16T14:01:08.618428Z","shell.execute_reply.started":"2024-06-16T14:00:49.731895Z","shell.execute_reply":"2024-06-16T14:01:08.617682Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff1e374c777e4f34ab2faaff595197c9"}},"metadata":{}}]},{"cell_type":"code","source":"# Ignore warnings\nlogging.set_verbosity(logging.CRITICAL)\n\n# Run text generation pipeline with our next model\nprompt = \"explain transformers?\"\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\nresult = pipe(f\"<s>[INST] {prompt} [/INST]\")\nprint(result[0]['generated_text'])","metadata":{"execution":{"iopub.status.busy":"2024-06-16T14:01:08.619469Z","iopub.execute_input":"2024-06-16T14:01:08.619724Z","iopub.status.idle":"2024-06-16T14:02:01.841297Z","shell.execute_reply.started":"2024-06-16T14:01:08.619701Z","shell.execute_reply":"2024-06-16T14:02:01.840317Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"<s>[INST] explain transformers? [/INST]  Transformers are a type of electrical device that transfers electrical energy from one circuit to another through electromagnetic induction. nobody knows for sure who invented the transformer, but it is believed to have been invented in the 1880s by a British engineer named William Thomson (later Lord Kelvin). The transformer is a passive device, meaning it doesn't consume any power to operate, and it relies on electromagnetic induction to transfer energy between two circuits.\n\nThe basic structure of a transformer consists of two coils of wire, known as the primary and secondary coils, which are wrapped around a common magnetic core. When an alternating current (AC) flows through the primary coil, it generates a magnetic field that induces an electromotive force (EMF) in the secondary coil. The EMF in the secondary\n","output_type":"stream"}]},{"cell_type":"code","source":"# Train model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-06-16T14:02:01.842629Z","iopub.execute_input":"2024-06-16T14:02:01.842998Z","iopub.status.idle":"2024-06-16T14:28:41.016129Z","shell.execute_reply.started":"2024-06-16T14:02:01.842964Z","shell.execute_reply":"2024-06-16T14:28:41.015281Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [125/125 26:07, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>1.510800</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.459500</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>1.318800</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.359600</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>1.417000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=125, training_loss=1.4131605529785156, metrics={'train_runtime': 1598.0883, 'train_samples_per_second': 0.626, 'train_steps_per_second': 0.078, 'total_flos': 8971066649149440.0, 'train_loss': 1.4131605529785156, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.model.save_pretrained(new_model)","metadata":{"execution":{"iopub.status.busy":"2024-06-16T14:28:41.017734Z","iopub.execute_input":"2024-06-16T14:28:41.018086Z","iopub.status.idle":"2024-06-16T14:28:41.224720Z","shell.execute_reply.started":"2024-06-16T14:28:41.018053Z","shell.execute_reply":"2024-06-16T14:28:41.223933Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Ignore warnings\nlogging.set_verbosity(logging.CRITICAL)\n\n# Run text generation pipeline with our next model\nprompt = \"explain transformers?\"\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\nresult = pipe(f\"<s>[INST] {prompt} [/INST]\")\nprint(result[0]['generated_text'])","metadata":{"execution":{"iopub.status.busy":"2024-06-16T14:28:41.225808Z","iopub.execute_input":"2024-06-16T14:28:41.226091Z","iopub.status.idle":"2024-06-16T14:29:33.879224Z","shell.execute_reply.started":"2024-06-16T14:28:41.226065Z","shell.execute_reply":"2024-06-16T14:29:33.878255Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"<s>[INST] explain transformers? [/INST] Transformers are a type of neural network architecture that is commonly used for natural language processing tasks, such as language translation, text classification, and language modeling. Transformers are based on attention mechanisms that allow the network to focus on specific parts of the input data when making predictions. This allows transformers to capture long-range dependencies in the input data, which is important for tasks such as language translation.\n\nTransformers consist of an encoder and a decoder. The encoder takes in a sequence of tokens (e.g. words or characters) and outputs a sequence of vectors that represent the input data. The decoder takes in the output of the encoder and generates a sequence of output tokens. The decoder also has an attention mechanism that allows it to focus on specific parts of the input data when generating each output token.\n\nTransformers are trained using a large dataset of input and output pairs\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}